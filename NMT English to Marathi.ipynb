{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c7436f",
   "metadata": {},
   "source": [
    "## Datset Link:- http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44876e6a",
   "metadata": {},
   "source": [
    "# Unzipping the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c51437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  mar-eng.zip\n",
      "  inflating: mar.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "!unzip mar-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff402a81",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8b1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model,load_model, model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea223f",
   "metadata": {},
   "source": [
    "# Data Exploration & Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b801cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mar.txt','r',encoding='utf8') as f:\n",
    "      data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab717bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data_list = data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f39f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45234"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncleaned_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bdd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data_list = data.split('\\n')\n",
    "uncleaned_data_list = uncleaned_data_list[:45233]\n",
    "\n",
    "english_words = []\n",
    "marathi_words = []\n",
    "\n",
    "cleaned_data_list = []\n",
    "\n",
    "for word in uncleaned_data_list:\n",
    "  english_words.append(word.split('\\t')[:-1][0])\n",
    "  marathi_words.append(word.split('\\t')[:-1][1])\n",
    "    \n",
    "language_data = pd.DataFrame(columns=['English','Marathi'])\n",
    "language_data['English'] = english_words\n",
    "language_data['Marathi'] = marathi_words\n",
    "language_data.to_csv('eng-mar.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981faedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Marathi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>जा.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>पळ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>धाव!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>पळा!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>धावा!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Marathi\n",
       "0     Go.     जा.\n",
       "1    Run!     पळ!\n",
       "2    Run!    धाव!\n",
       "3    Run!    पळा!\n",
       "4    Run!   धावा!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fd5206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45233, 45233)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_texts = language_data['English'].values\n",
    "marathi_texts = language_data['Marathi'].values\n",
    "\n",
    "len(english_texts), len(marathi_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa162307",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f585140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to lower case\n",
    "english_texts = [x.lower() for x in english_texts]\n",
    "marathi_texts = [x.lower() for x in marathi_texts]\n",
    "\n",
    "#removing inverted commas\n",
    "english_texts = [re.sub(\"'\",'',x) for x in english_texts]\n",
    "marathi_texts = [re.sub(\"'\",'',x) for x in marathi_texts]\n",
    "\n",
    "#function to remove punctuation\n",
    "def remove_punc(text_list):\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  removed_punc_text = []\n",
    "  for sent in text_list:\n",
    "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
    "    removed_punc_text.append(' '.join(sentance))\n",
    "  return removed_punc_text\n",
    "\n",
    "english_texts = remove_punc(english_texts)\n",
    "marathi_texts = remove_punc(marathi_texts)\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "removed_digits_text = []\n",
    "\n",
    "for sent in english_texts:\n",
    "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
    "  removed_digits_text.append(' '.join(sentance))\n",
    "    \n",
    "english_texts = removed_digits_text\n",
    "\n",
    "# removing the digits from the marathi sentances\n",
    "marathi_texts = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in marathi_texts]\n",
    "marathi_texts = [re.sub(\"[\\u200d]\",\"\",x) for x in marathi_texts]\n",
    "\n",
    "# removing the stating and ending whitespaces\n",
    "english_texts = [x.strip() for x in english_texts]\n",
    "marathi_texts = [x.strip() for x in marathi_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b1406",
   "metadata": {},
   "source": [
    " # Adding < start > & < end > token to every japanese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd0c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('start जा end', 'go')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the start and end words in the marathi sentances\n",
    "marathi_texts = [\"start \" + x + \" end\" for x in marathi_texts]\n",
    "# manipulated_marathi_text_\n",
    "marathi_texts[0], english_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06c426",
   "metadata": {},
   "source": [
    "# Train Test Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a2c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = english_texts\n",
    "Y = marathi_texts\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95b6605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40709, 4524)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c0945",
   "metadata": {},
   "source": [
    "## Maximum length of a sentence in train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660d31da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Max_length(data):\n",
    "  max_length_ = max([len(x.split(' ')) for x in data])\n",
    "  return max_length_\n",
    "#Training data\n",
    "max_length_english = Max_length(X_train)\n",
    "max_length_marathi = Max_length(y_train)\n",
    "#Test data\n",
    "max_length_english_test = Max_length(X_test)\n",
    "max_length_marathi_test = Max_length(y_test)\n",
    "max_length_marathi, max_length_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "309c8e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_marathi_test,max_length_english_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c45d3d",
   "metadata": {},
   "source": [
    "# Tokenizing Data using Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb538741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5643, 13587)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishTokenizer = Tokenizer()\n",
    "englishTokenizer.fit_on_texts(X_train)\n",
    "Eword2index = englishTokenizer.word_index\n",
    "vocab_size_source = len(Eword2index) + 1\n",
    "\n",
    "X_train = englishTokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length_english, padding='post')\n",
    "X_test = englishTokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length_english, padding='post')\n",
    "\n",
    "marathiTokenizer = Tokenizer()\n",
    "marathiTokenizer.fit_on_texts(y_train)\n",
    "Mword2index = marathiTokenizer.word_index\n",
    "vocab_size_target = len(Mword2index) + 1\n",
    "\n",
    "y_train = marathiTokenizer.texts_to_sequences(y_train)\n",
    "y_train = pad_sequences(y_train, maxlen=max_length_marathi, padding='post')\n",
    "y_test = marathiTokenizer.texts_to_sequences(y_test)\n",
    "y_test = pad_sequences(y_test, maxlen = max_length_marathi, padding='post')\n",
    "\n",
    "vocab_size_source, vocab_size_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7402acf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 86, 122,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n",
       " array([  1,  39, 105,   3,   5,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7cf22",
   "metadata": {},
   "source": [
    "# Saving the Data & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fcd988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('English - Marathi/NMT_data.pkl','wb') as f:\n",
    "  pkl.dump([X_train, y_train, X_test, y_test],f)\n",
    "with open('English - Marathi/NMT_Etokenizer.pkl','wb') as f:\n",
    "  pkl.dump([vocab_size_source, Eword2index, englishTokenizer], f)\n",
    "with open('English - Marathi/NMT_Mtokenizer.pkl', 'wb') as f:\n",
    "  pkl.dump([vocab_size_target, Mword2index, marathiTokenizer], f)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788b50f",
   "metadata": {},
   "source": [
    "# Loading Data & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ad803e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('English - Marathi/NMT_Etokenizer.pkl','rb') as f:\n",
    "  vocab_size_source,Eword2index,englishTokenizer = pkl.load(f)\n",
    "with open('English - Marathi/NMT_Mtokenizer.pkl','rb') as f:\n",
    "  vocab_size_target,Mword2index,marathiTokenizer = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f7024fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('English - Marathi/NMT_data.pkl','rb') as f:\n",
    "    X_train, y_train, X_test, y_test = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe484a3",
   "metadata": {},
   "source": [
    "# Pay Attention - Importing Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8e2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7806b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session() \n",
    "latent_dim = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca5f43",
   "metadata": {},
   "source": [
    "# Creating Our Main Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5741082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_length_english,)) \n",
    "enc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "# Set up the decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "plot_model(model, to_file='train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c5cf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 35, 500)      2821500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 35, 500), (N 2002000     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 35, 500), (N 2002000     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 500)    6793500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 35, 500), (N 2002000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 500),  2002000     embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 13587)  13600587    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 31,724,087\n",
      "Trainable params: 31,724,087\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd7d9d",
   "metadata": {},
   "source": [
    "# Compiling & Defining Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa4ca8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4587f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6bead18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1   39  105    3    5    2    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   1   46 1258 4333 1076    3    2    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   1   25  542  907   48    2    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   1   22  233    5    2    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   1   25  753   36 3155 5404 4334    2    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]]\n",
      "[[   1   39  105 ...    0    0    0]\n",
      " [   1   46 1258 ...    0    0    0]\n",
      " [   1   25  542 ...    0    0    0]\n",
      " ...\n",
      " [   1    9 1916 ...    0    0    0]\n",
      " [   1  567    4 ...    0    0    0]\n",
      " [   1  166  811 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])\n",
    "print(y_train[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67d943",
   "metadata": {},
   "source": [
    "## Checkpoint for saving model whenver validation loss improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "385ce535",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=\"english_to_marathi+attention.h5\", \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fadaf3",
   "metadata": {},
   "source": [
    "## A keyborad Interrupt was made during training because the accuracy already saturated for only 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f15f520",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 49s 269ms/step - loss: 1.0477 - accuracy: 0.8641 - val_loss: 0.7811 - val_accuracy: 0.8849\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78112, saving model to english_to_marathi+attention.h5\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 44s 268ms/step - loss: 0.7759 - accuracy: 0.8852 - val_loss: 0.6807 - val_accuracy: 0.8942\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78112 to 0.68071, saving model to english_to_marathi+attention.h5\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 44s 268ms/step - loss: 0.6686 - accuracy: 0.8958 - val_loss: 0.5933 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68071 to 0.59326, saving model to english_to_marathi+attention.h5\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 44s 267ms/step - loss: 0.5664 - accuracy: 0.9066 - val_loss: 0.5245 - val_accuracy: 0.9119\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59326 to 0.52446, saving model to english_to_marathi+attention.h5\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.4870 - accuracy: 0.9156 - val_loss: 0.4742 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52446 to 0.47423, saving model to english_to_marathi+attention.h5\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.4217 - accuracy: 0.9237 - val_loss: 0.4346 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.47423 to 0.43461, saving model to english_to_marathi+attention.h5\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.3663 - accuracy: 0.9315 - val_loss: 0.4135 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43461 to 0.41350, saving model to english_to_marathi+attention.h5\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.3185 - accuracy: 0.9385 - val_loss: 0.3830 - val_accuracy: 0.9311\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.41350 to 0.38300, saving model to english_to_marathi+attention.h5\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.2782 - accuracy: 0.9450 - val_loss: 0.3618 - val_accuracy: 0.9341\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.38300 to 0.36180, saving model to english_to_marathi+attention.h5\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.2430 - accuracy: 0.9507 - val_loss: 0.3495 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36180 to 0.34947, saving model to english_to_marathi+attention.h5\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 44s 267ms/step - loss: 0.2130 - accuracy: 0.9558 - val_loss: 0.3365 - val_accuracy: 0.9380\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.34947 to 0.33654, saving model to english_to_marathi+attention.h5\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.1869 - accuracy: 0.9604 - val_loss: 0.3272 - val_accuracy: 0.9398\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.33654 to 0.32719, saving model to english_to_marathi+attention.h5\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.1644 - accuracy: 0.9644 - val_loss: 0.3207 - val_accuracy: 0.9409\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32719 to 0.32067, saving model to english_to_marathi+attention.h5\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.1456 - accuracy: 0.9676 - val_loss: 0.3143 - val_accuracy: 0.9428\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.32067 to 0.31426, saving model to english_to_marathi+attention.h5\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 44s 267ms/step - loss: 0.1291 - accuracy: 0.9706 - val_loss: 0.3110 - val_accuracy: 0.9431\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.31426 to 0.31102, saving model to english_to_marathi+attention.h5\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.1156 - accuracy: 0.9731 - val_loss: 0.3078 - val_accuracy: 0.9439\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.31102 to 0.30776, saving model to english_to_marathi+attention.h5\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.1041 - accuracy: 0.9754 - val_loss: 0.3083 - val_accuracy: 0.9445\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30776\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.0939 - accuracy: 0.9772 - val_loss: 0.3064 - val_accuracy: 0.9450\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.30776 to 0.30645, saving model to english_to_marathi+attention.h5\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0857 - accuracy: 0.9787 - val_loss: 0.3079 - val_accuracy: 0.9452\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30645\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0785 - accuracy: 0.9801 - val_loss: 0.3066 - val_accuracy: 0.9452\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30645\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0725 - accuracy: 0.9812 - val_loss: 0.3063 - val_accuracy: 0.9460\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30645 to 0.30628, saving model to english_to_marathi+attention.h5\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.0674 - accuracy: 0.9823 - val_loss: 0.3085 - val_accuracy: 0.9458\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.30628\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0630 - accuracy: 0.9830 - val_loss: 0.3102 - val_accuracy: 0.9456\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.30628\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0594 - accuracy: 0.9836 - val_loss: 0.3110 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.30628\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0563 - accuracy: 0.9841 - val_loss: 0.3121 - val_accuracy: 0.9461\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30628\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.0537 - accuracy: 0.9846 - val_loss: 0.3142 - val_accuracy: 0.9466\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30628\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.3172 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30628\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 0.3170 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30628\n",
      "Epoch 29/50\n",
      " 13/163 [=>............................] - ETA: 38s - loss: 0.0362 - accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8b5bac005127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data = ([X_test, y_test[:,:-1]],y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/deeplearning/lib64/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:], \n",
    "                    epochs=50, \n",
    "                    callbacks=[checkpoint],\n",
    "                    batch_size=250,\n",
    "                    validation_data = ([X_test, y_test[:,:-1]],y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70544403",
   "metadata": {},
   "source": [
    "# Importing & Loading Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c7eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab56169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d470d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_model('english_to_marathi+attention.h5',custom_objects={'AttentionLayer': AttentionLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93917da5",
   "metadata": {},
   "source": [
    "# Creating Inference Model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62671f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=500\n",
    "# encoder inference\n",
    "encoder_inputs = model_loaded.input[0]  #loading encoder_inputs\n",
    "encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n",
    "#print(encoder_outputs.shape)\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(35,latent_dim))\n",
    "# Get the embeddings of the decoder sequence\n",
    "decoder_inputs = model_loaded.layers[3].output\n",
    "#print(decoder_inputs.shape)\n",
    "dec_emb_layer = model_loaded.layers[5]\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_lstm = model_loaded.layers[7]\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "#attention inference\n",
    "attn_layer = model_loaded.layers[8]\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "concate = model_loaded.layers[9]\n",
    "decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_dense = model_loaded.layers[10]\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])\n",
    "# decoder_model = Model(\n",
    "# [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "# [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67e88fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 500)    6793500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 500),  2002000     embedding_1[2][0]                \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 35, 500)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 500),  500500      input_6[0][0]                    \n",
      "                                                                 lstm_3[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[2][0]                     \n",
      "                                                                 attention_layer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 13587)  13600587    concat_layer[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 22,896,587\n",
      "Trainable params: 22,896,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7755e82",
   "metadata": {},
   "source": [
    "## Defining index2word for loaded tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58458043",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eindex2word = englishTokenizer.index_word\n",
    "Mindex2word = marathiTokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48aacf",
   "metadata": {},
   "source": [
    "# Decoder function for decoding Numerical output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07c2e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(input_seq):\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = Mword2index['start']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "          break\n",
    "        else:\n",
    "          sampled_token = Mindex2word[sampled_token_index]\n",
    "\n",
    "          if(sampled_token!='end'):\n",
    "              decoded_sentence += ' '+sampled_token\n",
    "\n",
    "              # Exit condition: either hit max length or find stop word.\n",
    "              if (sampled_token == 'end' or len(decoded_sentence.split()) >= (26-1)):\n",
    "                  stop_condition = True\n",
    "\n",
    "          # Update the target sequence (of length 1).\n",
    "          target_seq = np.zeros((1,1))\n",
    "          target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "          # Update internal states\n",
    "          e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17575010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2Marathi(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=Mword2index['start']) and i!=Mword2index['end']):\n",
    "        newString=newString+Mindex2word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def Seq2English(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+Eindex2word[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ae071b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:- tom walked to his office \n",
      "Original Marathi: टॉम आपल्या ऑफिसला चालत गेला \n",
      "Predicted Marathi:  टॉम त्याच्या ऑफिसला गेला\n",
      "\n",
      "\n",
      "English:- i want you near me \n",
      "Original Marathi: मला तू माझ्याजवळ हवा आहेस \n",
      "Predicted Marathi:  मला तुम्ही माझ्याजवळ हव्या आहात\n",
      "\n",
      "\n",
      "English:- i dont want to go outside \n",
      "Original Marathi: मला बाहेर नाही जायचंय \n",
      "Predicted Marathi:  मला बाहेर जायचं नाहीये\n",
      "\n",
      "\n",
      "English:- february th is northern territories day in japan \n",
      "Original Marathi: जपानमध्ये फेब्रुवारी हा उत्तर भूप्रदेश दिवस असतो \n",
      "Predicted Marathi:  जपानमध्ये हा एकेकाळी ब्रिटिश लोकं एक रोमन होता\n",
      "\n",
      "\n",
      "English:- you are my prisoner \n",
      "Original Marathi: तू माझा कैदी आहेस \n",
      "Predicted Marathi:  तुम्ही माझे कैदी आहात\n",
      "\n",
      "\n",
      "English:- its all my fault \n",
      "Original Marathi: ही सगळी माझी चूक आहे \n",
      "Predicted Marathi:  ही सगळी माझीच चूक आहे\n",
      "\n",
      "\n",
      "English:- show me your hands \n",
      "Original Marathi: हात दाखव \n",
      "Predicted Marathi:  तुझे हात दाखवा\n",
      "\n",
      "\n",
      "English:- he likes geography and history \n",
      "Original Marathi: त्याला भूगोल आणि इतिहास आवडतात \n",
      "Predicted Marathi:  त्याला भूगोल आणि इतिहास आवडतात\n",
      "\n",
      "\n",
      "English:- ten years have passed since he went to america \n",
      "Original Marathi: त्याला अमेरिकेला जाऊन दहा वर्ष झाली आहेत \n",
      "Predicted Marathi:  त्यांना जपानला येऊन पाच वर्ष झाली आहेत\n",
      "\n",
      "\n",
      "English:- why do they call you that \n",
      "Original Marathi: तुला त्या तसं का म्हणतात \n",
      "Predicted Marathi:  तुला ते तसं का म्हणतात\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):  \n",
    "  print(\"English:-\",Seq2English(X_test[i]))\n",
    "  print(\"Original Marathi:\",Seq2Marathi(y_test[i]))\n",
    "  print(\"Predicted Marathi:\",decode_output(X_test[i].reshape(1,35)))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b01660",
   "metadata": {},
   "source": [
    "# Defining a predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fae1a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text_list):\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  removed_punc_text = []\n",
    "  for sent in text_list:\n",
    "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
    "    removed_punc_text.append(' '.join(sentance))\n",
    "  return removed_punc_text\n",
    "\n",
    "def PredMarathi(statement):\n",
    "    english_text = [statement]\n",
    "    english_text = [x.lower() for x in english_text]\n",
    "    english_text = [re.sub(\"'\",'',x) for x in english_text]\n",
    "    english_text = remove_punc(english_text)\n",
    "    \n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    removed_digits_text = []\n",
    "    for sent in english_text:\n",
    "      sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
    "      removed_digits_text.append(' '.join(sentance))\n",
    "    english_text = removed_digits_text\n",
    "    english_text = [x.strip() for x in english_text]\n",
    "    \n",
    "    english_text = englishTokenizer.texts_to_sequences(english_text)\n",
    "    english_text = np.array(english_text)\n",
    "    english_text = pad_sequences(english_text, maxlen=35, padding='post')\n",
    "    # 35 is the length of the english sentence with maximum length!!!\n",
    "    print(\"Predicted Marathi:\",decode_sequence(english_text.reshape(1,35)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1eb5a",
   "metadata": {},
   "source": [
    "# Some Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ae0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Marathi:  मी चांगला मुलगा आहे\n"
     ]
    }
   ],
   "source": [
    "PredMarathi(\"I am a good boy!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6f96713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Marathi:  तू कशी आहेस\n"
     ]
    }
   ],
   "source": [
    "PredMarathi(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3831a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Marathi:  ती एक सुंदर मुलगी आहे\n"
     ]
    }
   ],
   "source": [
    "PredMarathi(\"She is a beautiful girl!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
